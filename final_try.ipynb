{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKING OPTICALFLOW FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: RoadAccidents\n",
      "Optical flow generated for class: RoadAccidents\n",
      "Processing class: RoadAccidents\n",
      "Optical flow generated for class: RoadAccidents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def compute_optical_flow_per_class(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Compute optical flow for all frames in the input directory and save in the output directory.\n",
    "    Args:\n",
    "        input_dir (str): Path to the input class folder (e.g., Train/Abuse).\n",
    "        output_dir (str): Path to the output class folder for optical flow.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    frame_files = sorted([f for f in os.listdir(input_dir) if f.endswith(\".png\")])\n",
    "    \n",
    "    prev_frame = cv2.imread(os.path.join(input_dir, frame_files[0]))\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    for i in range(1, len(frame_files)):\n",
    "        curr_frame = cv2.imread(os.path.join(input_dir, frame_files[i]))\n",
    "        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute optical flow\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        # Convert to HSV for visualization\n",
    "        hsv = np.zeros_like(prev_frame)\n",
    "        hsv[..., 1] = 255\n",
    "        hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "        hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        flow_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        # Save optical flow image\n",
    "        flow_filename = f\"flow_{i:04d}.png\"\n",
    "        cv2.imwrite(os.path.join(output_dir, flow_filename), flow_image)\n",
    "\n",
    "        prev_gray = curr_gray\n",
    "\n",
    "def generate_optical_flow_for_dataset(root_dir, output_root_dir, categories):\n",
    "    \"\"\"\n",
    "    Generate optical flow for the entire dataset organized by categories.\n",
    "    Args:\n",
    "        root_dir (str): Root directory of the dataset (e.g., Train or Test).\n",
    "        output_root_dir (str): Root directory to save optical flow frames.\n",
    "        categories (list): List of class names (subfolder names).\n",
    "    \"\"\"\n",
    "    for category in categories:\n",
    "        input_dir = os.path.join(root_dir, category)\n",
    "        output_dir = os.path.join(output_root_dir, category)\n",
    "        print(f\"Processing class: {category}\")\n",
    "        compute_optical_flow_per_class(input_dir, output_dir)\n",
    "        print(f\"Optical flow generated for class: {category}\")\n",
    "\n",
    "# Example usage\n",
    "categories = [ \"RoadAccidents\", ]\n",
    "\n",
    "# Generate optical flow for Train and Test sets\n",
    "generate_optical_flow_for_dataset(\"Train\", \"Train/OpticalFlow\", categories)\n",
    "generate_optical_flow_for_dataset(\"Test\", \"Test/OpticalFlow\", categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKING AND SAVING NPY FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without chunking complete files into npy format\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def save_as_numpy(root_dir, output_dir, categories, sequence_length, stride, transform):\n",
    "    \"\"\"\n",
    "    Preprocess frames and save them as .npy files for each class.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root directory of the dataset (e.g., Train or Test).\n",
    "        output_dir (str): Directory to save the .npy files.\n",
    "        categories (list): List of class names.\n",
    "        sequence_length (int): Number of frames in each sequence.\n",
    "        stride (int): Step size for creating overlapping sequences.\n",
    "        transform (callable): Transformations to apply to each frame.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(root_dir, category)\n",
    "        frame_files = sorted([os.path.join(category_path, f) for f in os.listdir(category_path) if f.endswith(\".png\")])\n",
    "        print(f\"Processing category: {category} ({len(frame_files)} frames)\")\n",
    "\n",
    "        sequences = []\n",
    "        for i in range(0, len(frame_files) - sequence_length + 1, stride):\n",
    "            frames = [transform(Image.open(frame).convert(\"RGB\")) for frame in frame_files[i:i + sequence_length]]\n",
    "            sequences.append(torch.stack(frames, dim=1).numpy())  # Convert to numpy array\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"{category}.npy\")\n",
    "        np.save(output_file, np.array(sequences))  # Save all sequences for the category\n",
    "        print(f\"Saved {len(sequences)} sequences to {output_file}\")\n",
    "\n",
    "# Define categories\n",
    "categories = [\"Abuse\", \"Arson\", \"Burglary\", \"Fighting\", \"Robbery\", \"Shoplifting\", \"Stealing\"]\n",
    "\n",
    "# Transformation for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Preprocess and save as .npy files\n",
    "save_as_numpy(root_dir=\"Train/OpticalFlow\", output_dir=\"Preprocessed/optical/Train\", categories=categories, sequence_length=16, stride=8, transform=transform)\n",
    "save_as_numpy(root_dir=\"Test/OpticalFlow\", output_dir=\"Preprocessed/optical/Test\", categories=categories, sequence_length=16, stride=8, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking the files into npy format\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def split_npy_into_chunks(input_file, output_dir, chunk_size):\n",
    "    \"\"\"\n",
    "    Split a large .npy file into smaller chunks.\n",
    "    Args:\n",
    "        input_file (str): Path to the input .npy file.\n",
    "        output_dir (str): Directory to save the smaller chunks.\n",
    "        chunk_size (int): Number of sequences per chunk.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = np.load(input_file)\n",
    "\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk = data[i:i + chunk_size]\n",
    "        chunk_file = os.path.join(output_dir, f\"{os.path.basename(input_file).replace('.npy', '')}_chunk_{i // chunk_size}.npy\")\n",
    "        np.save(chunk_file, chunk)\n",
    "        print(f\"Saved chunk {i // chunk_size} to {chunk_file}\")\n",
    "\n",
    "# Example Usage\n",
    "categories = [\"Abuse\", \"Arson\", \"Burglary\", \"Fighting\", \"Robbery\", \"Shoplifting\", \"Stealing\"]\n",
    "chunk_size = 1000  # Adjust based on your memory limits\n",
    "\n",
    "# Split train and test datasets\n",
    "for category in categories:\n",
    "    split_npy_into_chunks(\n",
    "        input_file=f\"Preprocessed/optical/Train/{category}.npy\",\n",
    "        output_dir=f\"PreprocessedChunks/OpticalTrain/{category}\",\n",
    "        chunk_size=chunk_size\n",
    "    )\n",
    "    split_npy_into_chunks(\n",
    "        input_file=f\"Preprocessed/optical/Test/{category}.npy\",\n",
    "        output_dir=f\"PreprocessedChunks/OpticalTest/{category}\",\n",
    "        chunk_size=chunk_size\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEGINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING FOR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class ChunkedNumpyDataset(Dataset):\n",
    "    def __init__(self, chunk_dir, categories):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_dir (str): Directory containing chunked .npy files.\n",
    "            categories (list): List of class names.\n",
    "        \"\"\"\n",
    "        self.chunks = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load all chunk paths and their sequence counts\n",
    "        for label, category in enumerate(categories):\n",
    "            category_dir = os.path.join(chunk_dir, category)\n",
    "            chunk_files = sorted([os.path.join(category_dir, f) for f in os.listdir(category_dir) if f.endswith(\".npy\")])\n",
    "            \n",
    "            for chunk_file in chunk_files:\n",
    "                sequences = np.load(chunk_file, mmap_mode=\"r\")  # Load sequence metadata without fully loading\n",
    "                self.chunks.append((chunk_file, len(sequences)))\n",
    "                self.labels.extend([label] * len(sequences))\n",
    "\n",
    "        # Precompute cumulative lengths for fast indexing\n",
    "        self.lengths = [chunk[1] for chunk in self.chunks]\n",
    "        self.cumulative_lengths = np.cumsum(self.lengths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.lengths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Identify which chunk the index belongs to\n",
    "        chunk_idx = np.searchsorted(self.cumulative_lengths, idx, side=\"right\")\n",
    "        relative_idx = idx if chunk_idx == 0 else idx - self.cumulative_lengths[chunk_idx - 1]\n",
    "\n",
    "        # Load the specific chunk and access the required sequence\n",
    "        chunk_file, _ = self.chunks[chunk_idx]\n",
    "        data = np.load(chunk_file, mmap_mode=\"r\")\n",
    "        sequence = torch.tensor(data[relative_idx])\n",
    "        label = self.labels[idx]\n",
    "        return sequence, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignedMultiStreamDataset(Dataset):\n",
    "    def __init__(self, rgb_dataset, flow_dataset):\n",
    "        \"\"\"\n",
    "        Combines RGB and Optical Flow datasets into a single aligned dataset.\n",
    "        Args:\n",
    "            rgb_dataset: Dataset object for RGB frames.\n",
    "            flow_dataset: Dataset object for Optical Flow frames.\n",
    "        \"\"\"\n",
    "        self.rgb_dataset = rgb_dataset\n",
    "        self.flow_dataset = flow_dataset\n",
    "        self.length = min(len(rgb_dataset), len(flow_dataset))  # Match lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve RGB and flow data\n",
    "        rgb_data, rgb_label = self.rgb_dataset[idx]\n",
    "        flow_data, flow_label = self.flow_dataset[idx]\n",
    "\n",
    "        # Ensure labels match\n",
    "        assert rgb_label == flow_label, f\"Label mismatch: RGB({rgb_label}) vs Flow({flow_label})\"\n",
    "\n",
    "        # Return separate tensors for RGB and Flow\n",
    "        return (rgb_data, flow_data), rgb_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "categories = [\"Abuse\", \"Arson\", \"Burglary\", \"Fighting\", \"Robbery\", \"Shoplifting\", \"Stealing\"]\n",
    "\n",
    "# Dataset directories\n",
    "rgb_train_dir = \"PreprocessedChunks/Train/\"\n",
    "flow_train_dir = \"PreprocessedChunks/OpticalTrain/\"\n",
    "rgb_test_dir = \"PreprocessedChunks/Test/\"\n",
    "flow_test_dir = \"PreprocessedChunks/OpticalTest/\"\n",
    "\n",
    "\n",
    "# Initialize datasets\n",
    "train_rgb_dataset = ChunkedNumpyDataset(chunk_dir=rgb_train_dir, categories=categories)\n",
    "train_flow_dataset = ChunkedNumpyDataset(chunk_dir=flow_train_dir, categories=categories)\n",
    "train_dataset = AlignedMultiStreamDataset(train_rgb_dataset, train_flow_dataset)\n",
    "\n",
    "\n",
    "test_rgb_dataset = ChunkedNumpyDataset(chunk_dir=rgb_test_dir, categories=categories)\n",
    "test_flow_dataset = ChunkedNumpyDataset(chunk_dir=flow_test_dir, categories=categories)\n",
    "test_dataset = AlignedMultiStreamDataset(test_rgb_dataset, test_flow_dataset)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiStreamR2Plus1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiStreamR2Plus1D, self).__init__()\n",
    "        \n",
    "        # RGB Stream\n",
    "        self.rgb_stream = r2plus1d_18(pretrained=True)\n",
    "        self.rgb_stream.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.rgb_stream.fc.in_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Optical Flow Stream\n",
    "        self.flow_stream = r2plus1d_18(pretrained=True)\n",
    "        self.flow_stream.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.flow_stream.fc.in_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Fusion Layer\n",
    "        self.fc_fusion = nn.Linear(2 * num_classes, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, rgb_input, flow_input):\n",
    "        # Forward pass through RGB and flow streams\n",
    "        rgb_out = self.rgb_stream(rgb_input)\n",
    "        flow_out = self.flow_stream(flow_input)\n",
    "\n",
    "        # Concatenate outputs and pass through fusion layer\n",
    "        combined_out = torch.cat((rgb_out, flow_out), dim=1)\n",
    "        return self.fc_fusion(combined_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_24324\\3471267753.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(init_scale=65536.0)\n",
      "Training Epoch 1/1:   0%|          | 0/1709 [00:00<?, ?it/s]C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_24324\\3471267753.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training Epoch 1/1:   2%|▏         | 36/1709 [23:46<18:24:57, 39.63s/it, loss=1.12]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m     50\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "num_classes = len(categories)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiStreamR2Plus1D(num_classes=num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "scaler = GradScaler(init_scale=65536.0)\n",
    "\n",
    "\n",
    "# Function to save model checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, scaler, save_dir=\"checkpoints\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict() if scaler is not None else None,\n",
    "    }\n",
    "    save_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Checkpoint saved at {save_path}\")\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "save_dir = \"checkpoints\"\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Progress bar for training\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch}/{epochs}\")\n",
    "    for (rgb_data, flow_data), labels in progress_bar:\n",
    "        rgb_data, flow_data, labels = rgb_data.to(device), flow_data.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with autocast():\n",
    "            outputs = model(rgb_data, flow_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch [{epoch}/{epochs}] Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    save_checkpoint(epoch, model, optimizer, scheduler, scaler, save_dir)\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "# Progress bar for evaluation\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "    for (rgb_data, flow_data), labels in progress_bar:\n",
    "        rgb_data, flow_data, labels = rgb_data.to(device), flow_data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            outputs = model(rgb_data, flow_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "# Print ev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler, scaler):\n",
    "    \"\"\"\n",
    "    Load a model checkpoint to resume training.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the saved checkpoint file.\n",
    "        model (torch.nn.Module): The model to load the weights into.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to load the state into.\n",
    "        scheduler (torch.optim.lr_scheduler): The scheduler to load the state into.\n",
    "        scaler (torch.cuda.amp.GradScaler): The gradient scaler to load the state into.\n",
    "    \n",
    "    Returns:\n",
    "        int: The epoch to resume training from.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    if scaler is not None and checkpoint['scaler_state_dict'] is not None:\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    print(f\"Checkpoint loaded: {checkpoint_path}\")\n",
    "    return checkpoint['epoch']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
