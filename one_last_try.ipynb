{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEGINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import r2plus1d_18\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING FOR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStreamDatasetRAM(Dataset):\n",
    "    def __init__(self, rgb_root, flow_root, categories, sequence_length=16, stride=8, transform=None):\n",
    "        \n",
    "        self.rgb_data = []\n",
    "        self.flow_data = []\n",
    "        self.labels = []\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "        for label, category in enumerate(categories):\n",
    "            # Load RGB frames\n",
    "            rgb_path = os.path.join(rgb_root, category)\n",
    "            rgb_files = sorted([os.path.join(rgb_path, f) for f in os.listdir(rgb_path) if f.endswith(\".png\")])\n",
    "\n",
    "            # Load optical flow frames\n",
    "            flow_path = os.path.join(flow_root, category)\n",
    "            flow_files = sorted([os.path.join(flow_path, f) for f in os.listdir(flow_path) if f.endswith(\".png\")])\n",
    "\n",
    "            # Align datasets by truncating to the shorter length\n",
    "            min_length = min(len(rgb_files), len(flow_files))\n",
    "            rgb_files = rgb_files[:min_length]\n",
    "            flow_files = flow_files[:min_length]\n",
    "\n",
    "            print(f\"Preloading category: {category} (RGB: {len(rgb_files)}, Flow: {len(flow_files)})\")\n",
    "\n",
    "            # Load and process sequences\n",
    "            rgb_frames = [self.transform(Image.open(f).convert(\"RGB\")) for f in rgb_files]\n",
    "            flow_frames = [self.transform(Image.open(f).convert(\"RGB\")) for f in flow_files]\n",
    "\n",
    "            for i in range(0, min_length - sequence_length + 1, stride):\n",
    "                self.rgb_data.append(rgb_frames[i:i + sequence_length])\n",
    "                self.flow_data.append(flow_frames[i:i + sequence_length])\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_sequence = torch.stack(self.rgb_data[idx], dim=1)  # Shape: (C, T, H, W)\n",
    "        flow_sequence = torch.stack(self.flow_data[idx], dim=1)  # Shape: (C, T, H, W)\n",
    "        label = self.labels[idx]\n",
    "        return (rgb_sequence, flow_sequence), label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading category: Shoplifting (RGB: 24834, Flow: 24834)\n",
      "Preloading category: RoadAccidents (RGB: 23485, Flow: 23485)\n",
      "Preloading category: Fighting (RGB: 24683, Flow: 24683)\n",
      "Train dataset loaded with 9121 sequences\n"
     ]
    }
   ],
   "source": [
    "# Define categories\n",
    "categories = [\"Shoplifting\", \"RoadAccidents\", \"Fighting\"]\n",
    "\n",
    "# Transformation for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "# Load multi-stream datasets into RAM\n",
    "train_dataset = MultiStreamDatasetRAM(\n",
    "    rgb_root=\"Train/\",\n",
    "    flow_root=\"OpticalFlowTrain/\",\n",
    "    categories=categories,\n",
    "    sequence_length=16,\n",
    "    stride=8,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train dataset loaded with {len(train_dataset)} sequences\")\n",
    "#print(f\"Test dataset loaded with {len(test_dataset)} sequences\")\n",
    "\n",
    "# DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16  # Adjust based on GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
    "\n",
    "class OptimizedMultiStreamR2Plus1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(OptimizedMultiStreamR2Plus1D, self).__init__()\n",
    "\n",
    "        # Shared stream (pretrained backbone)\n",
    "        self.shared_stream = r2plus1d_18(weights=R2Plus1D_18_Weights.DEFAULT)\n",
    "        in_features = self.shared_stream.fc.in_features  # Save in_features before replacing fc\n",
    "        self.shared_stream.fc = nn.Identity()  # Remove final FC layer\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.rgb_fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self.flow_fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fc_fusion = nn.Sequential(\n",
    "                nn.Linear(2 * num_classes, num_classes),\n",
    "                nn.Softmax(dim=1)  # Keep Sequential structure if needed\n",
    ")\n",
    "    def forward(self, rgb_input, flow_input):\n",
    "        \n",
    "\n",
    "        # Process inputs through the shared stream\n",
    "        rgb_features = self.shared_stream(rgb_input)\n",
    "        flow_features = self.shared_stream(flow_input)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        rgb_out = self.rgb_fc(rgb_features)\n",
    "        flow_out = self.flow_fc(flow_features)\n",
    "\n",
    "        # Concatenate and pass through fusion layer\n",
    "        combined_out = torch.cat((rgb_out, flow_out), dim=1)\n",
    "        \n",
    "        return self.fc_fusion(combined_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
    "\n",
    "class OptimizedMultiStreamR2Plus1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(OptimizedMultiStreamR2Plus1D, self).__init__()\n",
    "\n",
    "        # Shared stream (pretrained backbone)\n",
    "        self.shared_stream = r2plus1d_18(weights=R2Plus1D_18_Weights.DEFAULT)\n",
    "        in_features = self.shared_stream.fc.in_features  # Save in_features before replacing fc\n",
    "        self.shared_stream.fc = nn.Identity()  # Remove final FC layer\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.rgb_fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self.flow_fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fc_fusion = nn.Linear(2 * num_classes, num_classes)  # Direct Linear layer\n",
    "\n",
    "\n",
    "    def forward(self, rgb_input, flow_input):\n",
    "        rgb_features = self.shared_stream(rgb_input)\n",
    "        flow_features = self.shared_stream(flow_input)\n",
    "\n",
    "        rgb_out = self.rgb_fc(rgb_features)\n",
    "        flow_out = self.flow_fc(flow_features)\n",
    "\n",
    "        combined_out = torch.cat((rgb_out, flow_out), dim=1)\n",
    "        fused_out = self.fc_fusion(combined_out)  # Single Linear layer\n",
    "\n",
    "        return fused_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
    "\n",
    "class OptimizedMultiStreamR2Plus1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(OptimizedMultiStreamR2Plus1D, self).__init__()\n",
    "\n",
    "        # Shared stream (pretrained backbone)\n",
    "        self.shared_stream = r2plus1d_18(weights=R2Plus1D_18_Weights.DEFAULT)\n",
    "        in_features = self.shared_stream.fc.in_features  # Save in_features before replacing fc\n",
    "        self.shared_stream.fc = nn.Identity()  # Remove final FC layer\n",
    "\n",
    "        # Shared fully connected layer\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)  # Output logits for each modality\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fc_fusion = nn.Sequential(\n",
    "            nn.Linear(2 * num_classes, num_classes),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb_input, flow_input):\n",
    "        # Process inputs through shared backbone\n",
    "        rgb_features = self.shared_stream(rgb_input)\n",
    "        flow_features = self.shared_stream(flow_input)\n",
    "\n",
    "        # Process features through shared fully connected layer\n",
    "        rgb_out = self.shared_fc(rgb_features)\n",
    "        flow_out = self.shared_fc(flow_features)\n",
    "\n",
    "        # Concatenate outputs and pass through fusion layer\n",
    "        combined_out = torch.cat((rgb_out, flow_out), dim=1)\n",
    "        fused_out = self.fc_fusion(combined_out)\n",
    "\n",
    "        return fused_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2Plus1D(nn.Module):\n",
    "    \"\"\"A lightweight Conv2Plus1D block.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, spatial_kernel=(1, 3, 3), temporal_kernel=(3, 1, 1), stride=(1, 1, 1), padding=(1, 1, 1)):\n",
    "        super().__init__()\n",
    "        self.spatial_conv = nn.Conv3d(in_channels, out_channels, spatial_kernel, stride, padding, bias=False)\n",
    "        self.temporal_conv = nn.Conv3d(out_channels, out_channels, temporal_kernel, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.temporal_conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "\n",
    "class LightweightR2Plus1D(nn.Module):\n",
    "    \"\"\"A lightweight R(2+1)D backbone.\"\"\"\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.stem = Conv2Plus1D(in_channels, 64, spatial_kernel=(1, 7, 7), temporal_kernel=(3, 1, 1), stride=(1, 2, 2), padding=(0, 3, 3))\n",
    "        \n",
    "        self.layer1 = self._make_layer(64, 128, stride=(1, 1, 1))\n",
    "        self.layer2 = self._make_layer(128, 256, stride=(2, 2, 2))\n",
    "        self.layer3 = self._make_layer(256, 512, stride=(2, 2, 2))\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, stride):\n",
    "        return nn.Sequential(\n",
    "            Conv2Plus1D(in_channels, out_channels, stride=stride),\n",
    "            Conv2Plus1D(out_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class AttentionFusion(nn.Module):\n",
    "    \"\"\"Attention-based fusion layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(input_dim, output_dim)\n",
    "        self.key = nn.Linear(input_dim, output_dim)\n",
    "        self.value = nn.Linear(input_dim, output_dim)\n",
    "        self.fc = nn.Linear(output_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Cross-attention between two streams\n",
    "        query = self.query(x1)\n",
    "        key = self.key(x2)\n",
    "        value = self.value(x2)\n",
    "        \n",
    "        attention = F.softmax(torch.matmul(query, key.transpose(-1, -2)), dim=-1)\n",
    "        out = torch.matmul(attention, value)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "class ImprovedMultiStreamR2Plus1D(nn.Module):\n",
    "    \"\"\"Improved MultiStream R2Plus1D with modality interaction.\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.rgb_stream = LightweightR2Plus1D(3, 512)  # RGB stream\n",
    "        self.optical_flow_stream = LightweightR2Plus1D(3, 512)  # Optical flow stream\n",
    "        \n",
    "        self.attention_fusion = AttentionFusion(512, 512)  # Early modality interaction\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, rgb, optical_flow):\n",
    "        # Process each stream\n",
    "        rgb_features = self.rgb_stream(rgb)\n",
    "        optical_flow_features = self.optical_flow_stream(optical_flow)\n",
    "        \n",
    "        # Early interaction with attention-based fusion\n",
    "        fused_features = self.attention_fusion(rgb_features, optical_flow_features)\n",
    "        \n",
    "        # Final classification\n",
    "        return self.final_fc(fused_features)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = ImprovedMultiStreamR2Plus1D(num_classes=3)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightweightR2Plus1D(\n",
      "  (rgb_stem): Sequential(\n",
      "    (0): Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3))\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (flow_stem): Sequential(\n",
      "    (0): Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3))\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (rgb_layer1): Conv2Plus1D(\n",
      "    (temporal_conv): Conv3d(32, 64, kernel_size=(3, 1, 1), stride=(1, 2, 2), padding=(1, 0, 0), bias=False)\n",
      "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (spatial_conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "    (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (rgb_layer2): Conv2Plus1D(\n",
      "    (temporal_conv): Conv3d(64, 128, kernel_size=(3, 1, 1), stride=(1, 2, 2), padding=(1, 0, 0), bias=False)\n",
      "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (spatial_conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "    (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (flow_layer1): Conv2Plus1D(\n",
      "    (temporal_conv): Conv3d(32, 64, kernel_size=(3, 1, 1), stride=(1, 2, 2), padding=(1, 0, 0), bias=False)\n",
      "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (spatial_conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "    (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (flow_layer2): Conv2Plus1D(\n",
      "    (temporal_conv): Conv3d(64, 128, kernel_size=(3, 1, 1), stride=(1, 2, 2), padding=(1, 0, 0), bias=False)\n",
      "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (spatial_conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "    (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fusion_layer): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  (pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2Plus1D(nn.Module):\n",
    "    \"\"\"A lightweight Conv2Plus1D block.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=(1, 1, 1), padding=(1, 1, 1)):\n",
    "        super().__init__()\n",
    "        self.temporal_conv = nn.Conv3d(in_channels, out_channels, kernel_size=(3, 1, 1), stride=stride, padding=(1, 0, 0), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.spatial_conv = nn.Conv3d(out_channels, out_channels, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.temporal_conv(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.bn2(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class LightweightR2Plus1D(nn.Module):\n",
    "    \"\"\"Lightweight R(2+1)D-inspired model for RGB and Optical Flow.\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Shared stem for RGB and Optical Flow\n",
    "        self.rgb_stem = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.flow_stem = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Shared R(2+1)D blocks\n",
    "        self.rgb_layer1 = Conv2Plus1D(32, 64, stride=(1, 2, 2))\n",
    "        self.rgb_layer2 = Conv2Plus1D(64, 128, stride=(1, 2, 2))\n",
    "        \n",
    "        self.flow_layer1 = Conv2Plus1D(32, 64, stride=(1, 2, 2))\n",
    "        self.flow_layer2 = Conv2Plus1D(64, 128, stride=(1, 2, 2))\n",
    "\n",
    "        # Fusion and final classification\n",
    "        self.fusion_layer = nn.Conv3d(256, 128, kernel_size=(1, 1, 1), bias=False)  # Fuse RGB and Flow\n",
    "        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, rgb, flow):\n",
    "        # RGB stream\n",
    "        rgb = self.rgb_stem(rgb)\n",
    "        rgb = self.rgb_layer1(rgb)\n",
    "        rgb = self.rgb_layer2(rgb)\n",
    "\n",
    "        # Optical Flow stream\n",
    "        flow = self.flow_stem(flow)\n",
    "        flow = self.flow_layer1(flow)\n",
    "        flow = self.flow_layer2(flow)\n",
    "\n",
    "        # Fuse streams\n",
    "        x = torch.cat((rgb, flow), dim=1)  # Concatenate along channel dimension\n",
    "        x = self.fusion_layer(x)\n",
    "\n",
    "        # Global pooling and classification\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Example instantiation\n",
    "model = LightweightR2Plus1D(num_classes=3)  # Adjust `num_classes` as needed\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImprovedR2Plus1D(\n",
      "  (rgb_stem): Sequential(\n",
      "    (0): Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3))\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout3d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (flow_stem): Sequential(\n",
      "    (0): Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3))\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout3d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (rgb_layer1): Conv2Plus1D(\n",
      "    (temporal_conv): Conv3d(32, 64, kernel_size=(3, 1, 1), stride=(1, 2, 2), padding=(1, 0, 0), bias=False)\n",
      "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (spatial_conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "    (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (rgb_layer2): Conv2Plus1D(\n",
      "    (temporal_conv): Conv3d(64, 128, kernel_size=(3, 1, 1), stride=(1, 2, 2), padding=(1, 0, 0), bias=False)\n",
      "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (spatial_conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "    (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (flow_layer1): Conv2Plus1D(\n",
      "    (temporal_conv): Conv3d(32, 64, kernel_size=(3, 1, 1), stride=(1, 2, 2), padding=(1, 0, 0), bias=False)\n",
      "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (spatial_conv): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "    (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (flow_layer2): Conv2Plus1D(\n",
      "    (temporal_conv): Conv3d(64, 128, kernel_size=(3, 1, 1), stride=(1, 2, 2), padding=(1, 0, 0), bias=False)\n",
      "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (spatial_conv): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "    (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout3d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (fusion): Sequential(\n",
      "    (0): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2Plus1D(nn.Module):\n",
    "    \"\"\"R(2+1)D Block: Temporal convolution followed by spatial convolution.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=(1, 1, 1), padding=(1, 1, 1), groups=1):\n",
    "        super().__init__()\n",
    "        self.temporal_conv = nn.Conv3d(\n",
    "            in_channels, out_channels, kernel_size=(3, 1, 1), stride=stride, padding=(1, 0, 0), bias=False, groups=groups\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels, momentum=0.1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.spatial_conv = nn.Conv3d(\n",
    "            out_channels, out_channels, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False, groups=groups\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels, momentum=0.1)\n",
    "        self.dropout = nn.Dropout3d(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.temporal_conv(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ImprovedR2Plus1D(nn.Module):\n",
    "    \"\"\"Improved R(2+1)D Model for RGB + Optical Flow.\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stems for RGB and Optical Flow\n",
    "        self.rgb_stem = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(p=0.2)  # Regularization\n",
    "        )\n",
    "        self.flow_stem = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(p=0.2)\n",
    "        )\n",
    "\n",
    "        # R(2+1)D Blocks\n",
    "        self.rgb_layer1 = Conv2Plus1D(32, 64, stride=(1, 2, 2))\n",
    "        self.rgb_layer2 = Conv2Plus1D(64, 128, stride=(1, 2, 2))\n",
    "        \n",
    "        self.flow_layer1 = Conv2Plus1D(32, 64, stride=(1, 2, 2))\n",
    "        self.flow_layer2 = Conv2Plus1D(64, 128, stride=(1, 2, 2))\n",
    "\n",
    "        # Fusion Layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv3d(256, 128, kernel_size=(1, 1, 1)),  # Reduce dimensionality\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Global Pooling and Classifier\n",
    "        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb, flow):\n",
    "        # RGB Stream\n",
    "        rgb = self.rgb_stem(rgb)\n",
    "        rgb = self.rgb_layer1(rgb)\n",
    "        rgb = self.rgb_layer2(rgb)\n",
    "\n",
    "        # Optical Flow Stream\n",
    "        flow = self.flow_stem(flow)\n",
    "        flow = self.flow_layer1(flow)\n",
    "        flow = self.flow_layer2(flow)\n",
    "\n",
    "        # Late Fusion\n",
    "        x = torch.cat((rgb, flow), dim=1)  # Concatenate along channel dimension\n",
    "        x = self.fusion(x)\n",
    "\n",
    "        # Global Pooling and Classification\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Example instantiation\n",
    "model = ImprovedR2Plus1D(num_classes=3)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, scaler, save_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Save a model checkpoint including the model state, optimizer state,\n",
    "    scheduler state, scaler state (for mixed precision), and the current epoch.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): Current epoch number.\n",
    "        model (torch.nn.Module): The model to save.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer state to save.\n",
    "        scheduler (torch.optim.lr_scheduler): The scheduler state to save.\n",
    "        scaler (torch.cuda.amp.GradScaler): The GradScaler state for mixed precision.\n",
    "        save_dir (str): Directory to save checkpoints.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict() if scaler is not None else None,\n",
    "    }\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"checkpoint6_epoch_{epoch}.pth\")\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Checkpoint saved at {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_13736\\2368186611.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/10:   0%|          | 0/571 [00:00<?, ?it/s]C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_13736\\2368186611.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision\n",
      "Epoch 1/10: 100%|██████████| 571/571 [00:16<00:00, 34.63it/s, accuracy=0.551, loss=1.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.9232, Accuracy = 0.5507\n",
      "Checkpoint saved at checkpoints\\checkpoint6_epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 571/571 [00:15<00:00, 36.85it/s, accuracy=0.767, loss=0.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 0.5771, Accuracy = 0.7670\n",
      "Checkpoint saved at checkpoints\\checkpoint6_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 571/571 [00:15<00:00, 36.73it/s, accuracy=0.834, loss=0.67] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 0.4263, Accuracy = 0.8342\n",
      "Checkpoint saved at checkpoints\\checkpoint6_epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 571/571 [00:15<00:00, 36.94it/s, accuracy=0.864, loss=1.33]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 0.3577, Accuracy = 0.8640\n",
      "Checkpoint saved at checkpoints\\checkpoint6_epoch_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 571/571 [00:15<00:00, 36.95it/s, accuracy=0.89, loss=0.612]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.2949, Accuracy = 0.8896\n",
      "Checkpoint saved at checkpoints\\checkpoint6_epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 571/571 [00:15<00:00, 36.91it/s, accuracy=0.908, loss=1.88]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 0.2615, Accuracy = 0.9079\n",
      "Checkpoint saved at checkpoints\\checkpoint6_epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 571/571 [00:15<00:00, 36.85it/s, accuracy=0.919, loss=1.55]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss = 0.2341, Accuracy = 0.9190\n",
      "Checkpoint saved at checkpoints\\checkpoint6_epoch_7.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 571/571 [00:15<00:00, 37.00it/s, accuracy=0.919, loss=0.566] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss = 0.2244, Accuracy = 0.9189\n",
      "Checkpoint saved at checkpoints\\checkpoint6_epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 571/571 [00:15<00:00, 37.08it/s, accuracy=0.921, loss=0.318] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss = 0.2225, Accuracy = 0.9212\n",
      "Checkpoint saved at checkpoints\\checkpoint6_epoch_9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 571/571 [00:15<00:00, 36.14it/s, accuracy=0.926, loss=0.586] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 0.2104, Accuracy = 0.9260\n",
      "Checkpoint saved at checkpoints\\checkpoint6_epoch_10.pth\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Initialize model, weighted loss, optimizer, scheduler, and scaler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImprovedR2Plus1D(num_classes=len(categories)).to(device)\n",
    "class_weights = torch.tensor([1.0, 1.0, 1.0]).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)  # Weighted loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop with checkpoint saving\n",
    "epochs = 10\n",
    "save_dir = \"checkpoints\"\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "    for (rgb_data, flow_data), labels in progress_bar:\n",
    "        rgb_data, flow_data, labels = rgb_data.to(device), flow_data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():  # Mixed precision\n",
    "            outputs = model(rgb_data, flow_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=correct / total)\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch}: Loss = {running_loss / len(train_loader):.4f}, Accuracy = {correct / total:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(epoch, model, optimizer, scheduler, scaler, save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_24052\\3560897321.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_24052\\3560897321.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from checkpoints\\three\\checkpoint_epoch_8.pth\n"
     ]
    }
   ],
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler, scaler):\n",
    "    \"\"\"\n",
    "    Load a training checkpoint and resume training.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint {checkpoint_path} not found!\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    if scaler is not None and 'scaler_state_dict' in checkpoint and checkpoint['scaler_state_dict'] is not None:\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "    return checkpoint['epoch']\n",
    "\n",
    "\n",
    "# Initialize model, optimizer, scheduler, and scaler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OptimizedMultiStreamR2Plus1D(num_classes=len(categories)).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Load from checkpoint\n",
    "checkpoint_path = r\"checkpoints\\three\\checkpoint_epoch_8.pth\"\n",
    "start_epoch = load_checkpoint(checkpoint_path, model, optimizer, scheduler, scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading category: Shoplifting (RGB: 1230, Flow: 1230)\n",
      "Preloading category: RoadAccidents (RGB: 1230, Flow: 1230)\n",
      "Preloading category: Fighting (RGB: 1230, Flow: 1230)\n",
      "Test dataset loaded with 456 sequences\n"
     ]
    }
   ],
   "source": [
    "# Define categories\n",
    "categories = [\"Shoplifting\", \"RoadAccidents\", \"Fighting\"]\n",
    "\n",
    "# Transformation for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "test_dataset = MultiStreamDatasetRAM(\n",
    "    rgb_root=\"BalancedTest/\",\n",
    "    flow_root=\"OpticalBalancedTest/\",\n",
    "    categories=categories,\n",
    "    sequence_length=16,\n",
    "    stride=8,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Test dataset loaded with {len(test_dataset)} sequences\")\n",
    "\n",
    "# DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16  # Adjust based on GPU memory\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_13736\\495963320.py:49: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_13736\\495963320.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from checkpoints\\checkpoint6_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 29/29 [00:00<00:00, 35.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "{'Shoplifting': {'precision': 0.676923076923077, 'recall': 0.2894736842105263, 'f1-score': 0.4055299539170507, 'support': 152.0}, 'RoadAccidents': {'precision': 0.6736842105263158, 'recall': 0.8421052631578947, 'f1-score': 0.7485380116959064, 'support': 152.0}, 'Fighting': {'precision': 0.472636815920398, 'recall': 0.625, 'f1-score': 0.5382436260623229, 'support': 152.0}, 'accuracy': 0.5855263157894737, 'macro avg': {'precision': 0.607748034456597, 'recall': 0.5855263157894737, 'f1-score': 0.56410386389176, 'support': 456.0}, 'weighted avg': {'precision': 0.607748034456597, 'recall': 0.5855263157894737, 'f1-score': 0.56410386389176, 'support': 456.0}}\n",
      "Confusion Matrix:\n",
      "[[ 44  26  82]\n",
      " [  0 128  24]\n",
      " [ 21  36  95]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to load a checkpoint\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler, scaler=None):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    if scaler is not None and 'scaler_state_dict' in checkpoint:\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "        for inputs, labels in progress_bar:\n",
    "            # Assuming inputs is a tuple of (rgb_data, flow_data)\n",
    "            rgb_data, flow_data = inputs\n",
    "            rgb_data = rgb_data.to(device)\n",
    "            flow_data = flow_data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(rgb_data, flow_data)\n",
    "            _, preds = torch.max(outputs, 1)  # Get the predicted class\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(all_labels, all_preds, output_dict=True, target_names=categories)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return report, cm\n",
    "\n",
    "\n",
    "# Load the model and optimizer\n",
    "model = ImprovedR2Plus1D(num_classes=len(categories)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = r\"checkpoints\\checkpoint6_epoch_10.pth\"  # Replace with your checkpoint path\n",
    "load_checkpoint(checkpoint_path, model, optimizer, scheduler, scaler)\n",
    "\n",
    "# Evaluate the model\n",
    "report, cm = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAD-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB Shape: torch.Size([3, 16, 96, 96])\n",
      "Flow Shape: torch.Size([3, 16, 96, 96])\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the first sample from the dataset\n",
    "(inputs, label) = test_dataset[0]\n",
    "sample_rgb, sample_flow = inputs  # Unpack the RGB and Flow inputs\n",
    "\n",
    "print(f\"RGB Shape: {sample_rgb.shape}\")\n",
    "print(f\"Flow Shape: {sample_flow.shape}\")\n",
    "print(f\"Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Frame Shape: torch.Size([3, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "frame = sample_rgb[:, 0, :, :]  # First temporal frame (C, H, W)\n",
    "print(f\"Input Frame Shape: {frame.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB Shape: torch.Size([3, 16, 96, 96])\n",
      "Flow Shape: torch.Size([3, 16, 96, 96])\n",
      "Grad-CAM Shape (Before Aggregation): (7, 11, 11)\n",
      "Combined CAM Shape: (11, 11)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # Register hooks\n",
    "        self.target_layer.register_forward_hook(self.save_activations)\n",
    "        self.target_layer.register_backward_hook(self.save_gradients)\n",
    "\n",
    "    def save_activations(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def generate_cam(self, input_tensor, class_idx):\n",
    "        self.model.eval()\n",
    "        output = self.model(*input_tensor)\n",
    "        score = output[:, class_idx]\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        # Compute Grad-CAM\n",
    "        weights = torch.mean(self.gradients, dim=[2, 3, 4])  # Global average pooling\n",
    "        cam = torch.zeros_like(self.activations[0, 0])\n",
    "        for i, w in enumerate(weights[0]):\n",
    "            cam += w * self.activations[0, i]\n",
    "        cam = torch.relu(cam).detach().cpu().numpy()\n",
    "\n",
    "        # Squeeze to remove extra dimensions\n",
    "        cam = np.squeeze(cam)\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        return cam\n",
    "\n",
    "\n",
    "def visualize_grad_cam(cam, frame, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Visualize Grad-CAM on top of a frame.\n",
    "    Args:\n",
    "        cam: Grad-CAM heatmap (H, W).\n",
    "        frame: Original frame (C, H, W).\n",
    "        alpha: Transparency for overlay.\n",
    "    \"\"\"\n",
    "    # Aggregate activation maps if necessary\n",
    "    if len(cam.shape) == 3:  # If cam has multiple channels\n",
    "        cam = np.mean(cam, axis=0)  # Average across channels\n",
    "        print(f\"Combined CAM Shape: {cam.shape}\")  # Debugging output\n",
    "\n",
    "    # Normalize the heatmap\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)  # Normalize to [0, 1]\n",
    "\n",
    "    # Resize heatmap to match input frame dimensions\n",
    "    cam_resized = Image.fromarray(np.uint8(255 * cam)).resize((frame.shape[2], frame.shape[1]), Image.BILINEAR)\n",
    "    cam_resized = np.array(cam_resized)\n",
    "\n",
    "    # Convert the heatmap to RGB for blending\n",
    "    cam_resized_rgb = np.stack([cam_resized] * 3, axis=-1)  # Shape: (H, W, 3)\n",
    "\n",
    "    # Normalize the input frame to [0, 255]\n",
    "    frame = to_pil_image(frame)\n",
    "    frame = np.array(frame)\n",
    "\n",
    "    # Blend the heatmap (RGB) with the input frame\n",
    "    # Blend heatmap and frame\n",
    "    overlay = np.uint8(alpha * cam_resized_rgb + (1 - alpha) * frame)\n",
    "\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "# Step 1: Load a sample from the dataset\n",
    "(inputs, label) = test_dataset[0]\n",
    "sample_rgb, sample_flow = inputs\n",
    "\n",
    "print(f\"RGB Shape: {sample_rgb.shape}\")  # (C, T, H, W)\n",
    "print(f\"Flow Shape: {sample_flow.shape}\")  # (C, T, H, W)\n",
    "\n",
    "# Extract the first temporal frame\n",
    "frame = sample_rgb[:, 0, :, :]  # (C, H, W)\n",
    "\n",
    "# Step 2: Generate Grad-CAM\n",
    "grad_cam = GradCAM(model, model.rgb_stream.layer2[-1])  # Use layer2 instead of layer3\n",
    "cam = grad_cam.generate_cam((sample_rgb.unsqueeze(0).to(device), \n",
    "                             sample_flow.unsqueeze(0).to(device)), target_class)\n",
    "\n",
    "print(f\"Grad-CAM Shape (Before Aggregation): {cam.shape}\")  # (C, H, W)\n",
    "\n",
    "# Step 3: Visualize the Grad-CAM\n",
    "visualize_grad_cam(cam, frame)\n",
    "\n",
    "# Get model predictions for the input\n",
    "outputs = model(sample_rgb.unsqueeze(0).to(device), sample_flow.unsqueeze(0).to(device))\n",
    "predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE FOR BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights\n",
    "\n",
    "class OptimizedMultiStreamR2Plus1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(OptimizedMultiStreamR2Plus1D, self).__init__()\n",
    "\n",
    "        # Shared stream (pretrained backbone)\n",
    "        self.shared_stream = r2plus1d_18(weights=R2Plus1D_18_Weights.DEFAULT)\n",
    "        in_features = self.shared_stream.fc.in_features  # Save in_features before replacing fc\n",
    "        self.shared_stream.fc = nn.Identity()  # Remove final FC layer\n",
    "\n",
    "        # Shared fully connected layer\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)  # Output logits for each modality\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fc_fusion = nn.Sequential(\n",
    "            nn.Linear(2 * num_classes, num_classes),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb_input, flow_input):\n",
    "        # Process inputs through shared backbone\n",
    "        rgb_features = self.shared_stream(rgb_input)\n",
    "        flow_features = self.shared_stream(flow_input)\n",
    "\n",
    "        # Process features through shared fully connected layer\n",
    "        rgb_out = self.shared_fc(rgb_features)\n",
    "        flow_out = self.shared_fc(flow_features)\n",
    "\n",
    "        # Concatenate outputs and pass through fusion layer\n",
    "        combined_out = torch.cat((rgb_out, flow_out), dim=1)\n",
    "        fused_out = self.fc_fusion(combined_out)\n",
    "\n",
    "        return fused_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_13736\\3154701892.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedMultiStreamR2Plus1D(\n",
       "  (shared_stream): VideoResNet(\n",
       "    (stem): R2Plus1dStem(\n",
       "      (0): Conv3d(3, 45, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
       "      (1): BatchNorm3d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv3d(45, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "      (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(64, 144, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(144, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(64, 230, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(128, 230, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(230, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(128, 288, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(288, 128, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(128, 460, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(256, 460, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(460, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(460, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(256, 576, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(576, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(256, 921, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(512, 921, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(921, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(921, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2Plus1D(\n",
       "            (0): Conv3d(512, 1152, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "            (1): BatchNorm3d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv3d(1152, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "          )\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (shared_fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       "  (fc_fusion): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the categories\n",
    "categories = [\"Shoplifting\", \"RoadAccidents\", \"Fighting\"]\n",
    "\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OptimizedMultiStreamR2Plus1D(num_classes=len(categories)).to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = \"checkpoints/three/checkpoint_epoch_8.pth\"  # Replace with your path\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for resizing, normalizing, and converting frames to tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frames(frames, transform, sequence_length):\n",
    "    \n",
    "    processed_frames = [transform(Image.fromarray(frame)) for frame in frames]\n",
    "    if len(processed_frames) < sequence_length:\n",
    "        # Pad frames if there are fewer than sequence_length\n",
    "        while len(processed_frames) < sequence_length:\n",
    "            processed_frames.append(processed_frames[-1])\n",
    "    else:\n",
    "        processed_frames = processed_frames[:sequence_length]\n",
    "\n",
    "    sequence = torch.stack(processed_frames, dim=1)  # Shape: (C, T, H, W)\n",
    "    return sequence.unsqueeze(0)  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_single_video(video_path, model, transform, sequence_length, device, categories):\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert BGR (OpenCV format) to RGB and then to PIL Image\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "\n",
    "        # Apply transformations\n",
    "        frame = transform(frame)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Skip if no frames are available\n",
    "    if not frames:\n",
    "        print(f\"No frames found in video: {video_path}\")\n",
    "        return None\n",
    "\n",
    "    # Pad frames if they are fewer than sequence_length\n",
    "    if len(frames) < sequence_length:\n",
    "        print(f\"Padding video: {video_path} (not enough frames)\")\n",
    "        while len(frames) < sequence_length:\n",
    "            frames.append(frames[-1].clone())\n",
    "\n",
    "    # Convert frames to a tensor\n",
    "    video_tensor = torch.stack(frames[:sequence_length])  # Shape: (T, C, H, W)\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # Shape: (1, C, T, H, W)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        video_tensor = video_tensor.to(device)\n",
    "        outputs = model(video_tensor, video_tensor)  # Pass RGB and Flow (both same for demo)\n",
    "        _, predicted_class_idx = torch.max(outputs, 1)\n",
    "\n",
    "    # Map the predicted class index to the category name\n",
    "    predicted_category = categories[predicted_class_idx.item()]\n",
    "\n",
    "    return {\n",
    "        \"predicted_class_index\": predicted_class_idx.item(),\n",
    "        \"predicted_category\": predicted_category\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class Index: 2\n",
      "Predicted Category: Fighting\n"
     ]
    }
   ],
   "source": [
    "# Path to the single video file\n",
    "video_path = \"WhatsApp Video 2024-11-27 at 16.09.09_e0f0944c.mp4\"  # Replace with your video file path\n",
    "\n",
    "# Perform inference\n",
    "result = infer_single_video(video_path, model, transform, sequence_length=16, device=device, categories=categories)\n",
    "\n",
    "# Print the results\n",
    "if result:\n",
    "    print(f\"Predicted Class Index: {result['predicted_class_index']}\")\n",
    "    print(f\"Predicted Category: {result['predicted_category']}\")\n",
    "else:\n",
    "    print(\"Inference failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_single_video_with_logging(video_path, model, transform, sequence_length, device, categories):\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert BGR (OpenCV format) to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "\n",
    "        # Apply transformations\n",
    "        frame = transform(frame)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        raise ValueError(f\"No frames found in video: {video_path}\")\n",
    "\n",
    "    # Divide frames into sequences\n",
    "    sequences = []\n",
    "    for i in range(0, len(frames) - sequence_length + 1, sequence_length):\n",
    "        sequence = torch.stack(frames[i:i + sequence_length], dim=1)  # Shape: (C, T, H, W)\n",
    "        sequences.append(sequence.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "    # Log intermediate predictions\n",
    "    intermediate_predictions = []\n",
    "    for seq_idx, sequence in enumerate(sequences):\n",
    "        sequence = sequence.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(sequence, sequence)  # Pass RGB and Flow (both same for demo)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            predicted_class_idx = probabilities.argmax(dim=1).item()\n",
    "            predicted_category = categories[predicted_class_idx]\n",
    "\n",
    "            # Log prediction\n",
    "            print(f\"Sequence {seq_idx + 1}: Predicted Index: {predicted_class_idx}, \"\n",
    "                  f\"Predicted Category: {predicted_category}, \"\n",
    "                  f\"Probabilities: {probabilities.cpu().numpy()}\")\n",
    "\n",
    "            intermediate_predictions.append(predicted_category)\n",
    "\n",
    "    # Final prediction (e.g., majority voting)\n",
    "    final_prediction = max(set(intermediate_predictions), key=intermediate_predictions.count)\n",
    "\n",
    "    return {\n",
    "        \"final_prediction\": final_prediction,\n",
    "        \"intermediate_predictions\": intermediate_predictions\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 2: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 3: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 4: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 5: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 6: Predicted Index: 0, Predicted Category: Shoplifting, Probabilities: [[0.8422868  0.07475316 0.08296007]]\n",
      "Sequence 7: Predicted Index: 0, Predicted Category: Shoplifting, Probabilities: [[0.8487486  0.07170787 0.07954346]]\n",
      "Sequence 8: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 9: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 10: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 11: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 12: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 13: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 14: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 15: Predicted Index: 0, Predicted Category: Shoplifting, Probabilities: [[0.8496553  0.07128054 0.07906417]]\n",
      "Sequence 16: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 17: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 18: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 19: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 20: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 21: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.06632046 0.0622756  0.8714039 ]]\n",
      "Sequence 22: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 23: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 24: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 25: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 26: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 27: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 28: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 29: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "Sequence 30: Predicted Index: 2, Predicted Category: Fighting, Probabilities: [[0.32892197 0.31432545 0.3567526 ]]\n",
      "\n",
      "Final Prediction:\n",
      "Category: Fighting\n",
      "Intermediate Predictions: ['Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Shoplifting', 'Shoplifting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Shoplifting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting', 'Fighting']\n"
     ]
    }
   ],
   "source": [
    "video_path = \"WhatsApp Video 2024-11-27 at 16.09.09_e0f0944c.mp4\"\n",
    "categories = [\"Shoplifting\", \"RoadAccidents\", \"Fighting\"]\n",
    "\n",
    "result = infer_single_video_with_logging(video_path, model, transform, 16, device, categories)\n",
    "\n",
    "print(\"\\nFinal Prediction:\")\n",
    "print(f\"Category: {result['final_prediction']}\")\n",
    "print(f\"Intermediate Predictions: {result['intermediate_predictions']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
