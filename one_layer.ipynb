{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Import this for functional operations\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import GradScaler, autocast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Dataset Class with Preloading**\n",
    "class PreloadedDataset(Dataset):\n",
    "    def __init__(self, root_dir, categories, sequence_length=8, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing category folders.\n",
    "            categories (list): List of category names (subfolder names).\n",
    "            sequence_length (int): Number of consecutive frames in each sequence.\n",
    "            transform (callable, optional): Transform to apply to each frame.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "        for label, category in enumerate(categories):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            if not os.path.exists(category_path):\n",
    "                print(f\"Category folder does not exist: {category_path}\")\n",
    "                continue\n",
    "\n",
    "            # Load all PNGs into memory\n",
    "            print(f\"Preloading category: {category}\")\n",
    "            frame_files = sorted([f for f in os.listdir(category_path) if f.endswith(\".png\")])\n",
    "            for file in frame_files:\n",
    "                img = Image.open(os.path.join(category_path, file)).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                self.data.append(img)  # Add preprocessed image\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Create sequences on-the-fly from preloaded data\n",
    "        start_idx = idx\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "\n",
    "        # Handle edge cases by padding with zeros\n",
    "        if end_idx > len(self.data):\n",
    "            sequence = self.data[start_idx:] + [torch.zeros_like(self.data[0])] * (end_idx - len(self.data))\n",
    "        else:\n",
    "            sequence = self.data[start_idx:end_idx]\n",
    "\n",
    "        # Stack into tensor of shape (C, T, H, W)\n",
    "        sequence = torch.stack(sequence, dim=1)\n",
    "        label = self.labels[idx]\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Model Definition**\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Simple3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        flattened_feature_size = 16 * 4 * 56 * 56 \n",
    "        self.fc = nn.Linear(flattened_feature_size, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Prepare Datasets and DataLoaders**\n",
    "categories = [\"Abuse\", \"Arson\", \"Assault\", \"Burglary\", \"Explosion\", \"Fighting\",\n",
    "              \"NormalVideos\", \"RoadAccidents\", \"Robbery\", \"Shooting\", \"Shoplifting\",\n",
    "              \"Stealing\", \"Vandalism\"]\n",
    "\n",
    "train_root = \"Train\"\n",
    "test_root = \"Test\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = PreloadedDataset(root_dir=train_root, categories=categories, sequence_length=8, transform=transform)\n",
    "test_dataset = PreloadedDataset(root_dir=test_root, categories=categories, sequence_length=8, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)  # Preloading, so no workers\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_19272\\2264236428.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Mixed precision scaler\n"
     ]
    }
   ],
   "source": [
    "# **Model Setup**\n",
    "num_classes = len(categories)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Simple3DCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "scaler = GradScaler()  # Mixed precision scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Training Loop**\n",
    "num_epochs = 5\n",
    "save_dir = \"models\"  # Folder to save models\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Debug every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed with Average Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f\"simple_3dcnn_epoch_{epoch + 1}.pth\"))\n",
    "    print(f\"Model saved for Epoch {epoch + 1}\")\n",
    "\n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 13.01%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully and set to evaluation mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:42: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:42: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_8116\\3331029140.py:42: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  saved_model_path = \"model_final_simple\\simple_3dcnn_epoch_5.pth\"  # Replace with your actual saved file name\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_8116\\3331029140.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(saved_model_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define your model architecture (same as during training)\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes, sequence_length=8):\n",
    "        super(Simple3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "       \n",
    "        # Dynamically compute flattened feature size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, sequence_length, 112, 112)  # (batch_size, channels, sequence_length, height, width)\n",
    "            x = self.pool(torch.relu(self.conv1(dummy_input)))\n",
    "            self.flattened_feature_size = x.numel()\n",
    "\n",
    "        self.fc = nn.Linear(self.flattened_feature_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        #print(f\"Shape before flattening: {x.shape}\")  # Debug\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        #print(f\"Shape after flattening: {x.shape}\")  # Debug\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define categories and device\n",
    "categories = [\"Abuse\", \"Arson\", \"Assault\", \"Burglary\", \"Explosion\", \"Fighting\",\n",
    "              \"NormalVideos\", \"RoadAccidents\", \"Robbery\", \"Shooting\", \"Shoplifting\",\n",
    "              \"Stealing\", \"Vandalism\"]\n",
    "num_classes = len(categories)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = Simple3DCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "# Path to the saved model\n",
    "saved_model_path = \"model_final_simple\\simple_3dcnn_epoch_5.pth\"  # Replace with your actual saved file name\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(saved_model_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully and set to evaluation mode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIDEO PROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract frames from a video\n",
    "def extract_frames(video_path, sequence_length=8):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frame = transform(frame)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    # Split frames into sequences\n",
    "    frame_sequences = []\n",
    "    for i in range(0, len(frames) - sequence_length + 1, sequence_length):\n",
    "        frame_sequences.append(torch.stack(frames[i:i + sequence_length], dim=1))  # Shape: (C, T, H, W)\n",
    "\n",
    "    return frame_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_8116\\1281169443.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  model.load_state_dict(torch.load(\"model_final_simple\\simple_3dcnn_epoch_5.pth\"))  # Update with your saved model path\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_8116\\1281169443.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_final_simple\\simple_3dcnn_epoch_5.pth\"))  # Update with your saved model path\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = Simple3DCNN(num_classes=13)\n",
    "model.load_state_dict(torch.load(\"model_final_simple\\simple_3dcnn_epoch_5.pth\"))  # Update with your saved model path\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict_video(video_path):\n",
    "    frame_sequences = extract_frames(video_path, sequence_length=8)\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for frame_sequence in frame_sequences:\n",
    "            frame_sequence = frame_sequence.unsqueeze(0).to(device)  # Add batch dimension\n",
    "            outputs = model(frame_sequence)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.append(predicted.item())\n",
    "    # Return the most common predicted class\n",
    "    return max(set(predictions), key=predictions.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\R'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\R'\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_8116\\838924672.py:2: SyntaxWarning: invalid escape sequence '\\R'\n",
      "  video_path = \"output_folder_video\\RoadAccidents001_x264.avi\"  # Replace with your video path\n"
     ]
    }
   ],
   "source": [
    "# Run prediction\n",
    "video_path = \"output_folder_video\\RoadAccidents001_x264.avi\"  # Replace with your video path\n",
    "predicted_class = predict_video(video_path)\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: Stealing\n"
     ]
    }
   ],
   "source": [
    "category_mapping = [\"Abuse\", \"Arson\", \"Assault\", \"Burglary\", \"Explosion\",\n",
    "                    \"Fighting\", \"NormalVideos\", \"RoadAccidents\", \"Robbery\",\n",
    "                    \"Shooting\", \"Shoplifting\", \"Stealing\", \"Vandalism\"]\n",
    "print(f\"Predicted Category: {category_mapping[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
