{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Import this for functional operations\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import GradScaler, autocast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Dataset Class with Preloading**\n",
    "class PreloadedDataset(Dataset):\n",
    "    def __init__(self, root_dir, categories, sequence_length=8, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing category folders.\n",
    "            categories (list): List of category names (subfolder names).\n",
    "            sequence_length (int): Number of consecutive frames in each sequence.\n",
    "            transform (callable, optional): Transform to apply to each frame.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "        for label, category in enumerate(categories):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            if not os.path.exists(category_path):\n",
    "                print(f\"Category folder does not exist: {category_path}\")\n",
    "                continue\n",
    "\n",
    "            # Load all PNGs into memory\n",
    "            print(f\"Preloading category: {category}\")\n",
    "            frame_files = sorted([f for f in os.listdir(category_path) if f.endswith(\".png\")])\n",
    "            for file in frame_files:\n",
    "                img = Image.open(os.path.join(category_path, file)).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                self.data.append(img)  # Add preprocessed image\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Create sequences on-the-fly from preloaded data\n",
    "        start_idx = idx\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "\n",
    "        # Handle edge cases by padding with zeros\n",
    "        if end_idx > len(self.data):\n",
    "            sequence = self.data[start_idx:] + [torch.zeros_like(self.data[0])] * (end_idx - len(self.data))\n",
    "        else:\n",
    "            sequence = self.data[start_idx:end_idx]\n",
    "\n",
    "        # Stack into tensor of shape (C, T, H, W)\n",
    "        sequence = torch.stack(sequence, dim=1)\n",
    "        label = self.labels[idx]\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DualAttention3DCNNWithDropout(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DualAttention3DCNNWithDropout, self).__init__()\n",
    "        \n",
    "        # 3D Convolutional Layers\n",
    "        self.conv1 = nn.Conv3d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout3d(p=0.3)  # Dropout after first conv layer\n",
    "\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout3d(p=0.3)  # Dropout after second conv layer\n",
    "\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(128)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.dropout3 = nn.Dropout3d(p=0.3)  # Dropout after third conv layer\n",
    "\n",
    "        # Spatial Attention\n",
    "        self.spatial_fc = nn.Conv3d(128, 1, kernel_size=1)  # Reduce feature maps to attention weights\n",
    "        \n",
    "        # Temporal Attention\n",
    "        self.temporal_fc = nn.Linear(128, 1)  # Learn temporal attention weights\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(128, 128)  # Intermediate FC layer\n",
    "        self.dropout_fc = nn.Dropout(p=0.5)  # Dropout after FC layer\n",
    "        self.fc2 = nn.Linear(128, num_classes)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 3D CNN Layers\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout1(x)  # Apply dropout after first conv layer\n",
    "        \n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout2(x)  # Apply dropout after second conv layer\n",
    "        \n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.dropout3(x)  # Apply dropout after third conv layer\n",
    "\n",
    "        # Spatial Attention\n",
    "        spatial_weights = torch.sigmoid(self.spatial_fc(x))  # (B, 1, T, H, W)\n",
    "        x = x * spatial_weights  # Apply spatial attention\n",
    "\n",
    "        # Global Spatial Pooling\n",
    "        x = x.mean(dim=[3, 4])  # Reduce spatial dimensions (B, 128, T)\n",
    "\n",
    "        # Temporal Attention\n",
    "        x = x.permute(0, 2, 1)  # (B, T, 128)\n",
    "        temporal_weights = F.softmax(self.temporal_fc(x), dim=1)  # (B, T, 1)\n",
    "        x = (x * temporal_weights).sum(dim=1)  # Weighted sum over time (B, 128)\n",
    "\n",
    "        # Fully Connected Layers with Dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)  # Apply dropout after FC layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Prepare Datasets and DataLoaders**\n",
    "categories = [\"Abuse\", \"Arson\", \"Assault\", \"Burglary\", \"Explosion\", \"Fighting\",\n",
    "               \"RoadAccidents\", \"Robbery\", \"Shooting\", \"Shoplifting\",\n",
    "              \"Stealing\", \"Vandalism\"]\n",
    "\n",
    "train_root = \"Train\"\n",
    "test_root = \"Test\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),  # Slight rotations to avoid distortion\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.Resize((112, 112)),  # Ensure the size is consistent\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading category: Abuse\n",
      "Preloading category: Arson\n",
      "Preloading category: Assault\n",
      "Preloading category: Burglary\n",
      "Preloading category: Explosion\n",
      "Preloading category: Fighting\n",
      "Preloading category: RoadAccidents\n",
      "Preloading category: Robbery\n",
      "Preloading category: Shooting\n",
      "Preloading category: Shoplifting\n",
      "Preloading category: Stealing\n",
      "Preloading category: Vandalism\n",
      "Preloading category: Abuse\n",
      "Preloading category: Arson\n",
      "Preloading category: Assault\n",
      "Preloading category: Burglary\n",
      "Preloading category: Explosion\n",
      "Preloading category: Fighting\n",
      "Preloading category: RoadAccidents\n",
      "Preloading category: Robbery\n",
      "Preloading category: Shooting\n",
      "Preloading category: Shoplifting\n",
      "Preloading category: Stealing\n",
      "Preloading category: Vandalism\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Define dataset parameters\n",
    "sequence_length = 16\n",
    "val_split = 0.2  # Percentage of training data for validation\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "# Load the full train dataset\n",
    "full_train_dataset = PreloadedDataset(root_dir=train_root, categories=categories, sequence_length=sequence_length, transform=transform)\n",
    "\n",
    "# Compute sizes for train-validation split\n",
    "val_size = int(len(full_train_dataset) * val_split)\n",
    "train_size = len(full_train_dataset) - val_size\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = PreloadedDataset(root_dir=test_root, categories=categories, sequence_length=sequence_length, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_20104\\413351530.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Mixed precision scaler\n"
     ]
    }
   ],
   "source": [
    "# **Model Setup**\n",
    "num_classes = len(categories)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "model = DualAttention3DCNNWithDropout(num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "scaler = GradScaler()  # Mixed precision scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Training Loop**\n",
    "num_epochs = 5\n",
    "save_dir = \"models\"  # Folder to save models\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/7305 [00:00<?, ?it/s]C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_12396\\1021516613.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/5: 100%|██████████| 7305/7305 [28:22<00:00,  4.29it/s, loss=2.11]  \n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_12396\\1021516613.py:45: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed with Train Loss: 2.0874, Train Accuracy: 0.3204\n",
      "Validation Loss: 1.8198, Validation Accuracy: 0.4435\n",
      "Model saved for Epoch 1\n",
      "Starting Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 7305/7305 [30:28<00:00,  3.99it/s, loss=1.99]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 completed with Train Loss: 1.8954, Train Accuracy: 0.4125\n",
      "Validation Loss: 1.6535, Validation Accuracy: 0.5221\n",
      "Model saved for Epoch 2\n",
      "Starting Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 7305/7305 [6:00:38<00:00,  2.96s/it, loss=1.35]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 completed with Train Loss: 1.7751, Train Accuracy: 0.4719\n",
      "Validation Loss: 1.5320, Validation Accuracy: 0.5846\n",
      "Model saved for Epoch 3\n",
      "Starting Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 7305/7305 [29:37<00:00,  4.11it/s, loss=1.67]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 completed with Train Loss: 1.6850, Train Accuracy: 0.5155\n",
      "Validation Loss: 1.4185, Validation Accuracy: 0.6335\n",
      "Model saved for Epoch 4\n",
      "Starting Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 7305/7305 [30:28<00:00,  3.99it/s, loss=1.68] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 completed with Train Loss: 1.6151, Train Accuracy: 0.5511\n",
      "Validation Loss: 1.3351, Validation Accuracy: 0.6740\n",
      "Model saved for Epoch 5\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed with Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f\"simple_3dcnn_epoch_{epoch + 1}.pth\"))\n",
    "    print(f\"Model saved for Epoch {epoch + 1}\")\n",
    "\n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_12396\\1474588085.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.7687, Test Accuracy: 0.1663\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'epoch': epoch\n",
    "}, os.path.join(save_dir, f\"checkpoint_epoch_{epoch + 1}.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_20104\\1788839069.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  checkpoint = torch.load(os.path.join(\"models\\checkpoint_epoch_5.pth\"))\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_20104\\1788839069.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(\"models\\checkpoint_epoch_5.pth\"))\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(os.path.join(\"models\\checkpoint_epoch_5.pth\"))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming Training: Epoch 6/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_20104\\1687803831.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed with Train Loss: 1.5609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_20104\\1687803831.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.2988\n",
      "Resuming Training: Epoch 7/13\n",
      "Epoch 7 completed with Train Loss: 1.5450\n",
      "Validation Loss: 1.3034\n",
      "Resuming Training: Epoch 8/13\n",
      "Epoch 8 completed with Train Loss: 1.5394\n",
      "Validation Loss: 1.2942\n",
      "Resuming Training: Epoch 9/13\n",
      "Epoch 9 completed with Train Loss: 1.5309\n",
      "Validation Loss: 1.2801\n",
      "Resuming Training: Epoch 10/13\n",
      "Epoch 10 completed with Train Loss: 1.5231\n",
      "Validation Loss: 1.2807\n",
      "Resuming Training: Epoch 11/13\n",
      "Epoch 11 completed with Train Loss: 1.5173\n",
      "Validation Loss: 1.2630\n",
      "Resuming Training: Epoch 12/13\n",
      "Epoch 12 completed with Train Loss: 1.5164\n",
      "Validation Loss: 1.2662\n",
      "Resuming Training: Epoch 13/13\n",
      "Epoch 13 completed with Train Loss: 1.5148\n",
      "Validation Loss: 1.2652\n"
     ]
    }
   ],
   "source": [
    "num_additional_epochs = 8\n",
    "save_dir = \"models\" \n",
    "for epoch in range(start_epoch, start_epoch + num_additional_epochs):\n",
    "    print(f\"Resuming Training: Epoch {epoch + 1}/{start_epoch + num_additional_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} completed with Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }, os.path.join(save_dir, f\"checkpoint_epoch_{epoch + 1}.pth\"))\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.7938, Test Accuracy: 0.1508\n"
     ]
    }
   ],
   "source": [
    "# Test Phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Calculate average test loss and accuracy\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 32.71%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_20104\\2363634794.py:8: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  checkpoint_path = \"models\\checkpoint_epoch_13.pth\"  # Path to your checkpoint file\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_20104\\2363634794.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Reinitialize the model\n",
    "num_classes = 12  # Number of classes in your dataset\n",
    "model = DualAttention3DCNNWithDropout(num_classes=num_classes)\n",
    "\n",
    "# Load the saved model checkpoint\n",
    "checkpoint_path = \"models\\checkpoint_epoch_13.pth\"  # Path to your checkpoint file\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load the model's state dictionary\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "print(\"Model loaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIDEO PROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def preprocess_video(video_path, sequence_length=16):\n",
    "    \"\"\"\n",
    "    Preprocess a video file into a tensor suitable for the model.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        sequence_length (int): Number of frames to extract.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed video tensor of shape (1, C, T, H, W).\n",
    "    \"\"\"\n",
    "    # Define transformations (resize, normalize)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((112,112)),  # Resize frames to 224x224\n",
    "        transforms.ToTensor(),         # Convert to tensor\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "    ])\n",
    "    \n",
    "    # Open video file using OpenCV\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened() and frame_count < sequence_length:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        frame = Image.fromarray(frame)  # Convert to PIL image\n",
    "        frame = transform(frame)  # Apply transformations\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # If the video has fewer frames than sequence_length, pad with black frames\n",
    "    while len(frames) < sequence_length:\n",
    "        frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "    # Stack frames into a single tensor: (C, T, H, W)\n",
    "    video_tensor = torch.stack(frames, dim=1).unsqueeze(0)  # Add batch dimension\n",
    "    return video_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\R'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\R'\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_20104\\2088080384.py:20: SyntaxWarning: invalid escape sequence '\\R'\n",
      "  video_path = \"output_folder_video\\RoadAccidents001_x264.avi\"  # Path to the input video\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class Index: 10\n"
     ]
    }
   ],
   "source": [
    "def predict_video(model, video_tensor, device):\n",
    "    \"\"\"\n",
    "    Predict the class of a video using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        video_tensor (torch.Tensor): Preprocessed video tensor of shape (1, C, T, H, W).\n",
    "        device (torch.device): Device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted class index.\n",
    "    \"\"\"\n",
    "    video_tensor = video_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(video_tensor)  # Forward pass\n",
    "        _, predicted_class = torch.max(outputs, 1)  # Get the predicted class index\n",
    "    return predicted_class.item()\n",
    "\n",
    "# Example usage\n",
    "video_path = \"output_folder_video\\RoadAccidents001_x264.avi\"  # Path to the input video\n",
    "video_tensor = preprocess_video(video_path, sequence_length=16)\n",
    "predicted_class = predict_video(model, video_tensor, device)\n",
    "\n",
    "print(f\"Predicted Class Index: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Stealing\n"
     ]
    }
   ],
   "source": [
    "# Define your class label mapping\n",
    "class_labels = [\"Abuse\", \"Arson\", \"Assault\", \"Burglary\", \"Explosion\", \n",
    "                \"Fighting\", \"RoadAccidents\", \"Robbery\", \"Shooting\", \n",
    "                \"Shoplifting\", \"Stealing\", \"Vandalism\"]\n",
    "\n",
    "# Print the predicted class label\n",
    "print(f\"Predicted Class: {class_labels[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
