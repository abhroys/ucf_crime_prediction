{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset root (e.g., 'Train/').\n",
    "            transform (callable, optional): Optional transforms for frames.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.videos = []  # List of videos with their frame paths and labels\n",
    "\n",
    "        # Traverse the root directory and organize frames by video\n",
    "        print(\"Initializing dataset...\")\n",
    "        for label in os.listdir(root_dir):  # Class labels (e.g., Fighting, Shoplifting)\n",
    "            class_dir = os.path.join(root_dir, label)\n",
    "            if os.path.isdir(class_dir):\n",
    "                # Group frames by video prefix (e.g., Fighting001, Fighting002)\n",
    "                video_frames = {}\n",
    "                for frame_name in os.listdir(class_dir):  # List all frames in class folder\n",
    "                    prefix = \"_\".join(frame_name.split(\"_\")[:-1])  # Extract video prefix\n",
    "                    if prefix not in video_frames:\n",
    "                        video_frames[prefix] = []\n",
    "                    video_frames[prefix].append(os.path.join(class_dir, frame_name))\n",
    "\n",
    "                # Sort frames for each video by frame index\n",
    "                for prefix, frames in video_frames.items():\n",
    "                    frames = sorted(\n",
    "                        frames, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])  # Sort by frame index\n",
    "                    )\n",
    "                    self.videos.append((frames, label))  # (list of frame paths, label)\n",
    "\n",
    "        print(f\"Dataset initialized. Found {len(self.videos)} videos.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames, label = self.videos[idx]\n",
    "\n",
    "        # Load frames as PIL images\n",
    "        images = [Image.open(frame).convert(\"RGB\") for frame in frames]\n",
    "\n",
    "        if self.transform:\n",
    "            images = [self.transform(img) for img in images]\n",
    "\n",
    "        # Map labels to integers\n",
    "        label_map = {\"Fighting\": 0, \"Shoplifting\": 1, \"RoadAccidents\": 2}\n",
    "        label_tensor = torch.tensor(label_map[label])\n",
    "\n",
    "        return torch.stack(images), label_tensor  # Return sequence of frames, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),          # Convert to Tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Dataset initialized. Found 201 videos.\n",
      "\n",
      "Inspecting first few videos...\n",
      "\n",
      "Batch 1:\n",
      "Video Frames Shape: torch.Size([1, 117, 3, 64, 64]) (Batch Size, Frames, Channels, Height, Width)\n",
      "Label: tensor([2])\n",
      "\n",
      "Batch 2:\n",
      "Video Frames Shape: torch.Size([1, 143, 3, 64, 64]) (Batch Size, Frames, Channels, Height, Width)\n",
      "Label: tensor([1])\n",
      "\n",
      "Batch 3:\n",
      "Video Frames Shape: torch.Size([1, 460, 3, 64, 64]) (Batch Size, Frames, Channels, Height, Width)\n",
      "Label: tensor([2])\n"
     ]
    }
   ],
   "source": [
    "dataset = FrameDataset(root_dir=\"Train\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print(\"\\nInspecting first few videos...\")\n",
    "for i, (video_frames, label) in enumerate(dataloader):\n",
    "    print(f\"\\nBatch {i + 1}:\")\n",
    "    print(f\"Video Frames Shape: {video_frames.shape} (Batch Size, Frames, Channels, Height, Width)\")\n",
    "    print(f\"Label: {label}\")\n",
    "    if i == 2:  # Inspect only the first 3 videos\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded CNN backbone (ResNet18). Output feature size: 512\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "\n",
    "# Load a pre-trained ResNet18\n",
    "cnn_model = models.resnet18(pretrained=True)\n",
    "cnn_model.fc = nn.Identity()  # Remove the final classification layer\n",
    "cnn_model = cnn_model.cuda()  # Move the model to GPU\n",
    "\n",
    "print(\"\\nLoaded CNN backbone (ResNet18). Output feature size:\", 512)\n",
    "\n",
    "def extract_spatial_features(video_frames, cnn_model):\n",
    "    \n",
    "    batch_size, num_frames, c, h, w = video_frames.size()\n",
    "\n",
    "    # Reshape to process each frame independently\n",
    "    video_frames = video_frames.view(batch_size * num_frames, c, h, w)\n",
    "    features = cnn_model(video_frames)  # Extract features for all frames\n",
    "\n",
    "    # Reshape back to (batch_size, num_frames, feature_dim)\n",
    "    features = features.view(batch_size, num_frames, -1)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mask(sequences):\n",
    "  \n",
    "    # Pad the sequences to the length of the longest sequence\n",
    "    padded_batch = pad_sequence(sequences, batch_first=True)  # Shape: (batch_size, max_seq_len, feature_dim)\n",
    "\n",
    "    # Create a mask: True for padding, False for valid tokens\n",
    "    batch_size = len(sequences)\n",
    "    max_seq_len = padded_batch.size(1)\n",
    "    mask = torch.ones((batch_size, max_seq_len), dtype=torch.bool)  # Shape: (batch_size, max_seq_len)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        mask[i, :seq.size(0)] = False  # Valid tokens are False, padding is True\n",
    "\n",
    "    # Debugging mask shape\n",
    "\n",
    "    return padded_batch, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, feature_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = nn.Parameter(torch.zeros(1, max_len, feature_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.encoding[:, :seq_len, :]\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
    "      \n",
    "        super(TemporalLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,  # Input shape: (batch_size, seq_len, input_dim)\n",
    "            dropout=0.1,       # Dropout between LSTM layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)  # Fully connected layer for classification\n",
    "\n",
    "    def forward(self, features):\n",
    "    \n",
    "\n",
    "\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(features)  # lstm_out shape: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        last_hidden_state = lstm_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Fully connected layer\n",
    "        logits = self.fc(last_hidden_state)  # (batch_size, num_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_7776\\3336350846.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for video 51: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_7776\\3336350846.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for video 74: 2\n",
      "Label for video 154: 2\n",
      "Label for video 178: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Backward pass with scaling\u001b[39;00m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 35\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m     37\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model\n",
    "feature_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "num_classes = 3\n",
    "model = TemporalLSTM(feature_dim, num_heads, num_layers, num_classes).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for video_frames, labels in dataloader:\n",
    "        video_frames, labels = video_frames.cuda(), labels.cuda()\n",
    "\n",
    "        # Extract spatial features using the CNN\n",
    "        features = extract_spatial_features(video_frames, cnn_model)\n",
    "\n",
    "        # Pad sequences\n",
    "        padded_features, _ = create_mask([f.squeeze(0) for f in features])\n",
    "        padded_features = padded_features.cuda()\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # Enable mixed precision\n",
    "            outputs = model(padded_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass with scaling\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}], Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, file_path):\n",
    "    \"\"\"\n",
    "    Save a checkpoint of the model for continued training.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to save.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used during training.\n",
    "        epoch (int): The current epoch.\n",
    "        loss (float): The training loss at the time of saving.\n",
    "        file_path (str): The path to save the checkpoint file.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)\n",
    "    print(f\"Checkpoint saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to lstm/checkpoint_epoch30.pth\n"
     ]
    }
   ],
   "source": [
    "# Assume `model`, `optimizer`, `epoch`, and `loss` are defined\n",
    "save_checkpoint(model, optimizer, epoch, loss, file_path=\"lstm/checkpoint_epoch.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, file_path=\"lstm/model_checkpoint2.pth\"):\n",
    "    \"\"\"\n",
    "    Load a saved model checkpoint for continued training.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to load the checkpoint into.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to load the checkpoint into.\n",
    "        file_path (str): The path to the checkpoint file.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: epoch (int), loss (float)\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(file_path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    loss = checkpoint[\"loss\"]\n",
    "    print(f\"Checkpoint loaded from {file_path}, starting from epoch {epoch + 1}\")\n",
    "    return epoch, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_7776\\3813297673.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from lstm/checkpoint_epoch2.pth, starting from epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_7776\\3948509162.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 91.5762\n",
      "Epoch [2], Loss: 90.1781\n",
      "Epoch [3], Loss: 88.3611\n",
      "Epoch [4], Loss: 85.4649\n",
      "Epoch [5], Loss: 85.0240\n",
      "Epoch [6], Loss: 82.4540\n",
      "Epoch [7], Loss: 81.9375\n",
      "Epoch [8], Loss: 80.9263\n",
      "Epoch [9], Loss: 79.2760\n",
      "Epoch [10], Loss: 79.9016\n",
      "Epoch [11], Loss: 78.3686\n",
      "Epoch [12], Loss: 77.8196\n",
      "Epoch [13], Loss: 76.9582\n",
      "Epoch [14], Loss: 76.4198\n",
      "Epoch [15], Loss: 76.3869\n",
      "Epoch [16], Loss: 75.3072\n",
      "Epoch [17], Loss: 75.2737\n",
      "Epoch [18], Loss: 74.5243\n",
      "Epoch [19], Loss: 71.4722\n",
      "Epoch [20], Loss: 73.8407\n",
      "Epoch [21], Loss: 71.3456\n",
      "Epoch [22], Loss: 73.0596\n",
      "Epoch [23], Loss: 70.6635\n",
      "Epoch [24], Loss: 70.3186\n",
      "Epoch [25], Loss: 69.8441\n",
      "Epoch [26], Loss: 69.3912\n",
      "Epoch [27], Loss: 71.1430\n",
      "Epoch [28], Loss: 69.9398\n",
      "Epoch [29], Loss: 74.6160\n",
      "Epoch [30], Loss: 71.5581\n"
     ]
    }
   ],
   "source": [
    "# Assume `model` and `optimizer` are already defined\n",
    "start_epoch, prev_loss = load_checkpoint(model, optimizer, file_path=\"lstm/checkpoint_epoch2.pth\")\n",
    "\n",
    "# Resume training\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for video_frames, labels in dataloader:\n",
    "        video_frames, labels = video_frames.cuda(), labels.cuda()\n",
    "\n",
    "        # Extract spatial features using the CNN\n",
    "        features = extract_spatial_features(video_frames, cnn_model)\n",
    "\n",
    "        # Pad sequences\n",
    "        padded_features, _ = create_mask([f.squeeze(0) for f in features])\n",
    "        padded_features = padded_features.cuda()\n",
    "\n",
    "        with torch.cuda.amp.autocast():  # Enable mixed precision\n",
    "            outputs = model(padded_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass with scaling\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Training loop...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Dataset initialized. Found 49 videos.\n",
      "\n",
      "Test Accuracy: 0.1837\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Fighting       0.08      0.60      0.14         5\n",
      " Shoplifting       0.00      0.00      0.00        21\n",
      "       Other       0.55      0.26      0.35        23\n",
      "\n",
      "    accuracy                           0.18        49\n",
      "   macro avg       0.21      0.29      0.16        49\n",
      "weighted avg       0.26      0.18      0.18        49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\adars\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\adars\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1836734693877551,\n",
       " '              precision    recall  f1-score   support\\n\\n    Fighting       0.08      0.60      0.14         5\\n Shoplifting       0.00      0.00      0.00        21\\n       Other       0.55      0.26      0.35        23\\n\\n    accuracy                           0.18        49\\n   macro avg       0.21      0.29      0.16        49\\nweighted avg       0.26      0.18      0.18        49\\n')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, test_dataloader, cnn_model, device):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on the test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model.\n",
    "        test_dataloader: DataLoader for the test dataset.\n",
    "        cnn_model: Pre-trained CNN used for feature extraction.\n",
    "        device: 'cuda' or 'cpu' for computation.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    cnn_model.eval()  # Set the CNN to evaluation mode\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for video_frames, labels in test_dataloader:\n",
    "            video_frames, labels = video_frames.to(device), labels.to(device)\n",
    "            \n",
    "            # Extract spatial features using the CNN\n",
    "            features = extract_spatial_features(video_frames, cnn_model)\n",
    "            \n",
    "            # Pad sequences and create mask\n",
    "            padded_features, mask = create_mask([f.squeeze(0) for f in features])\n",
    "            padded_features = padded_features.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(padded_features)\n",
    "            _, preds = torch.max(outputs, dim=1)  # Get class predictions\n",
    "            \n",
    "            # Collect labels and predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Fighting\", \"Shoplifting\", \"Other\"])\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "# Example: Evaluate the model\n",
    "test_dataset = FrameDataset(root_dir=\"Test\", transform=transform)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Create test DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "evaluate_model(model, test_dataloader, cnn_model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
