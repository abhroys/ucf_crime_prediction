{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision.models.video import r3d_18\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class PreloadedDatasetWithStrideRAM(Dataset):\n",
    "    def __init__(self, root_dir, categories, sequence_length=16, stride=8, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing category folders.\n",
    "            categories (list): List of category names.\n",
    "            sequence_length (int): Number of consecutive frames in each sequence.\n",
    "            stride (int): Step size for overlapping sequences.\n",
    "            transform (callable): Transformation to apply to each frame.\n",
    "        \"\"\"\n",
    "        self.data = []  # Store all sequences in RAM\n",
    "        self.labels = []  # Corresponding labels for each sequence\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "        # Preload all frames into RAM\n",
    "        frame_cache = {}\n",
    "        for label, category in enumerate(categories):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            frame_files = sorted([os.path.join(category_path, f) for f in os.listdir(category_path) if f.endswith(\".png\")])\n",
    "            print(f\"Preloading category: {category} ({len(frame_files)} frames)\")\n",
    "\n",
    "            # Load all frames for this category into RAM\n",
    "            frames = [self.transform(Image.open(frame).convert(\"RGB\")) for frame in frame_files]\n",
    "            frame_cache[category] = frames\n",
    "\n",
    "            # Create sequences with stride\n",
    "            for i in range(0, len(frames) - sequence_length + 1, stride):\n",
    "                self.data.append(frames[i:i + sequence_length])  # Add sequence of frames (already preloaded)\n",
    "                self.labels.append(label)  # Add label for the sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the sequence and label\n",
    "        sequence = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Stack frames into a tensor of shape (C, T, H, W)\n",
    "        return torch.stack(sequence, dim=1), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading category: Abuse (19076 frames)\n",
      "Preloading category: Arson (24421 frames)\n",
      "Preloading category: Burglary (39504 frames)\n",
      "Preloading category: Explosion (18753 frames)\n",
      "Preloading category: Fighting (24684 frames)\n",
      "Preloading category: RoadAccidents (23486 frames)\n",
      "Preloading category: Robbery (41493 frames)\n",
      "Preloading category: Shoplifting (24835 frames)\n",
      "Preloading category: Stealing (44802 frames)\n",
      "Total training sequences: 32619\n"
     ]
    }
   ],
   "source": [
    "# Define categories\n",
    "categories = [\"Abuse\", \"Arson\", \"Burglary\", \"Explosion\", \n",
    "              \"Fighting\", \"RoadAccidents\", \"Robbery\", \n",
    "              \"Shoplifting\", \"Stealing\"]\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Load the dataset into RAM\n",
    "train_dataset = PreloadedDatasetWithStrideRAM(\n",
    "    root_dir=\"Train\",\n",
    "    categories=categories,\n",
    "    sequence_length=16,\n",
    "    stride=8,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Total training sequences: {len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created successfully.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define DataLoader parameters\n",
    "batch_size = 32  # Adjust based on GPU memory\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)  # No workers since data is in RAM\n",
    "\n",
    "print(\"DataLoader created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers:\n",
      "layer3.0.conv1.0.weight\n",
      "layer3.0.conv1.1.weight\n",
      "layer3.0.conv1.1.bias\n",
      "layer3.0.conv2.0.weight\n",
      "layer3.0.conv2.1.weight\n",
      "layer3.0.conv2.1.bias\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.1.conv1.0.weight\n",
      "layer3.1.conv1.1.weight\n",
      "layer3.1.conv1.1.bias\n",
      "layer3.1.conv2.0.weight\n",
      "layer3.1.conv2.1.weight\n",
      "layer3.1.conv2.1.bias\n",
      "layer4.0.conv1.0.weight\n",
      "layer4.0.conv1.1.weight\n",
      "layer4.0.conv1.1.bias\n",
      "layer4.0.conv2.0.weight\n",
      "layer4.0.conv2.1.weight\n",
      "layer4.0.conv2.1.bias\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.1.conv1.0.weight\n",
      "layer4.1.conv1.1.weight\n",
      "layer4.1.conv1.1.bias\n",
      "layer4.1.conv2.0.weight\n",
      "layer4.1.conv2.1.weight\n",
      "layer4.1.conv2.1.bias\n",
      "fc.1.weight\n",
      "fc.1.bias\n",
      "Model setup complete with dropout, class weights, and weight decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_10576\\614922912.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler('cuda')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import r3d_18\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Categories in your dataset\n",
    "categories = [\"Abuse\", \"Arson\", \"Burglary\", \"Explosion\", \n",
    "              \"Fighting\", \"RoadAccidents\", \"Robbery\", \n",
    "              \"Shoplifting\", \"Stealing\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# Load the pretrained ResNet3D model\n",
    "model = r3d_18(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer to match the number of classes\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),  # Add dropout before the final layer\n",
    "    nn.Linear(model.fc.in_features, num_classes)\n",
    ")\n",
    "\n",
    "# Move the model to GPU/CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Unfreeze layer2, layer3, layer4, and fc layers\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer2\" in name or \"layer3\" in name or \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True  # Trainable\n",
    "    else:\n",
    "        param.requires_grad = False  # Frozen\n",
    "\n",
    "# Print trainable layers to verify\n",
    "print(\"Trainable layers:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "\n",
    "# Class imbalance handling with CrossEntropyLoss\n",
    "# Define weights based on the number of frames in each category\n",
    "class_counts = [19076, 24421, 39504, 18753, 24684, 23486, 41493, 24835, 44802]  # Replace with your actual counts\n",
    "class_weights = torch.tensor([sum(class_counts) / c for c in class_counts], dtype=torch.float).to(device)\n",
    "\n",
    "# Use weighted CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = optim.AdamW([\n",
    "    {\"params\": model.layer2.parameters(), \"lr\": 0.0001},  # Lower LR for layer2\n",
    "    {\"params\": model.layer3.parameters(), \"lr\": 0.0001},  # Lower LR for layer3\n",
    "    {\"params\": model.layer4.parameters(), \"lr\": 0.0001},  # Lower LR for layer4\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 0.0005},      # Higher LR for fc\n",
    "], weight_decay=1e-3)\n",
    "\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Mixed Precision Training\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "print(\"Model setup complete with dropout, class weights, and weight decay.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_10576\\2843940055.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/1020 [00:00<?, ?it/s]C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_10576\\2843940055.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training Epoch 1: 100%|██████████| 1020/1020 [06:16<00:00,  2.71it/s, loss=0.00395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0374, Train Accuracy = 0.9886\n",
      "Model checkpoint saved at checkpoints\\resnet3d_checkpoint_epoch_1.pth\n",
      "Starting Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1020/1020 [06:27<00:00,  2.63it/s, loss=0.00354] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.0188, Train Accuracy = 0.9945\n",
      "Model checkpoint saved at checkpoints\\resnet3d_checkpoint_epoch_2.pth\n",
      "Starting Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1020/1020 [06:31<00:00,  2.61it/s, loss=0.00643] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.0135, Train Accuracy = 0.9954\n",
      "Model checkpoint saved at checkpoints\\resnet3d_checkpoint_epoch_3.pth\n",
      "Starting Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1020/1020 [06:05<00:00,  2.79it/s, loss=0.00127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.0245, Train Accuracy = 0.9929\n",
      "Model checkpoint saved at checkpoints\\resnet3d_checkpoint_epoch_4.pth\n",
      "Starting Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1020/1020 [06:41<00:00,  2.54it/s, loss=0.0167]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.0025, Train Accuracy = 0.9993\n",
      "Model checkpoint saved at checkpoints\\resnet3d_checkpoint_epoch_5.pth\n",
      "Starting Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 1020/1020 [06:05<00:00,  2.79it/s, loss=0.000141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.0008, Train Accuracy = 0.9998\n",
      "Model checkpoint saved at checkpoints\\resnet3d_checkpoint_epoch_6.pth\n",
      "Starting Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 1020/1020 [06:36<00:00,  2.57it/s, loss=6.04e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.0004, Train Accuracy = 0.9998\n",
      "Model checkpoint saved at checkpoints\\resnet3d_checkpoint_epoch_7.pth\n",
      "Starting Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1020/1020 [06:05<00:00,  2.79it/s, loss=4.6e-5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.0004, Train Accuracy = 0.9999\n",
      "Model checkpoint saved at checkpoints\\resnet3d_checkpoint_epoch_8.pth\n",
      "Starting Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1020/1020 [06:42<00:00,  2.53it/s, loss=0.0028]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.0004, Train Accuracy = 0.9998\n",
      "Model checkpoint saved at checkpoints\\resnet3d_checkpoint_epoch_9.pth\n",
      "Starting Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1020/1020 [06:22<00:00,  2.67it/s, loss=1.97e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.0003, Train Accuracy = 0.9999\n",
      "Model checkpoint saved at checkpoints\\resnet3d_checkpoint_epoch_10.pth\n",
      "Starting Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11:   3%|▎         | 27/1020 [00:08<05:17,  3.12it/s, loss=7.86e-5] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     21\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m, in \u001b[0;36mPreloadedDatasetWithStrideRAM.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Stack frames into a tensor of shape (C, T, H, W)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, label\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Initialize GradScaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 20\n",
    "save_dir = \"checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Ensure save directory exists\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"Starting Epoch {epoch}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    # Training loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch}\")\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision forward pass\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Scaled backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Update running metrics\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    save_path = os.path.join(save_dir, f\"resnet3d_checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model checkpoint saved at {save_path}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTLOADER AND MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading category: Abuse (297 frames)\n",
      "Loading category: Arson (2793 frames)\n",
      "Loading category: Burglary (7657 frames)\n",
      "Loading category: Explosion (6510 frames)\n",
      "Loading category: Fighting (1231 frames)\n",
      "Loading category: RoadAccidents (2663 frames)\n",
      "Loading category: Robbery (835 frames)\n",
      "Loading category: Shoplifting (7623 frames)\n",
      "Loading category: Stealing (1984 frames)\n",
      "Test dataset contains 1970 sequences.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_dir, categories, sequence_length=16, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing category folders.\n",
    "            categories (list): List of category names (subfolder names).\n",
    "            sequence_length (int): Number of consecutive frames in each sequence.\n",
    "            transform (callable, optional): Transformations to apply to each frame.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "        for label, category in enumerate(categories):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            frame_files = sorted([os.path.join(category_path, f) for f in os.listdir(category_path) if f.endswith(\".png\")])\n",
    "            print(f\"Loading category: {category} ({len(frame_files)} frames)\")\n",
    "\n",
    "            # Create non-overlapping sequences\n",
    "            for i in range(0, len(frame_files) - sequence_length + 1, sequence_length):\n",
    "                sequence = frame_files[i:i + sequence_length]\n",
    "                self.data.append(sequence)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load a sequence of frames\n",
    "        sequence = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Apply transformations to each frame\n",
    "        frames = [self.transform(Image.open(frame).convert(\"RGB\")) for frame in sequence]\n",
    "        return torch.stack(frames, dim=1), label  # Shape: (C, T, H, W)\n",
    "\n",
    "# Define the categories in the test dataset\n",
    "categories = [\"Abuse\", \"Arson\", \"Burglary\", \"Explosion\", \n",
    "              \"Fighting\", \"RoadAccidents\", \"Robbery\", \n",
    "              \"Shoplifting\", \"Stealing\"]\n",
    "\n",
    "# Path to the test dataset\n",
    "test_root = \"Test\"\n",
    "\n",
    "# Define transformations for test data\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),  # Resize to match the model input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize values\n",
    "])\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = TestDataset(root_dir=test_root, categories=categories, sequence_length=16, transform=test_transform)\n",
    "\n",
    "# Create the DataLoader for testing\n",
    "batch_size = 32  # Adjust as per your system's memory capacity\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Test dataset contains {len(test_dataset)} sequences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 3.4985, Test Accuracy = 0.2563\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Mixed precision inference (optional)\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "# Calculate test metrics\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f\"Test Loss = {test_loss:.4f}, Test Accuracy = {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING MODEL AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_10576\\241007383.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from checkpoints/resnet3d_checkpoint_epoch_10.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.video import r3d_18\n",
    "\n",
    "# Define the categories (used to initialize the model)\n",
    "categories = [\"Abuse\", \"Arson\", \"Burglary\", \"Explosion\", \n",
    "              \"Fighting\", \"RoadAccidents\", \"Robbery\", \n",
    "              \"Shoplifting\", \"Stealing\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# Initialize the model\n",
    "model = r3d_18(pretrained=False)  # Pretrained=False because we're loading weights\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(model.fc.in_features, num_classes)\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path = \"checkpoints/resnet3d_checkpoint_epoch_10.pth\"  # Replace with your file\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Load model weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(f\"Checkpoint loaded from {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 4.9459, Test Accuracy = 0.1756\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test dataset\n",
    "model.eval()  # Set model to evaluation mode\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "# Ensure no gradients are calculated during evaluation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)  # Compute test loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "# Calculate test metrics\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = test_correct / test_total\n",
    "\n",
    "print(f\"Test Loss = {test_loss:.4f}, Test Accuracy = {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALYSIS OF DATA AND METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2978\n",
      "Recall: 0.1756\n",
      "F1-Score: 0.1659\n",
      "Confusion Matrix:\n",
      "[[  1   0   0   0   1   9   6   0   1]\n",
      " [ 52  48  30   0   2  30   6   0   6]\n",
      " [  7  13  48  45   1 155 112   0  97]\n",
      " [ 71  14  17  44  18 147  67   6  22]\n",
      " [  4   2   6   0  24   6  20  12   2]\n",
      " [  0   8   1   2  13 124   6   4   8]\n",
      " [  2   0   0   0   0  13  22   0  15]\n",
      " [ 14   1  17   2   2   5 382   1  52]\n",
      " [ 29   3   1   0   0  36  21   0  34]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Abuse       0.01      0.06      0.01        18\n",
      "        Arson       0.54      0.28      0.37       174\n",
      "     Burglary       0.40      0.10      0.16       478\n",
      "    Explosion       0.47      0.11      0.18       406\n",
      "     Fighting       0.39      0.32      0.35        76\n",
      "RoadAccidents       0.24      0.75      0.36       166\n",
      "      Robbery       0.03      0.42      0.06        52\n",
      "  Shoplifting       0.04      0.00      0.00       476\n",
      "     Stealing       0.14      0.27      0.19       124\n",
      "\n",
      "     accuracy                           0.18      1970\n",
      "    macro avg       0.25      0.26      0.19      1970\n",
      " weighted avg       0.30      0.18      0.17      1970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate the model on test dataset\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report for detailed per-class metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
